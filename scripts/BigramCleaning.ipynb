{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[9]') \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalcounts = spark.read.option('header', False)\\\n",
    "                     .option('lineSep', '\\t')\\\n",
    "                     .csv('/data/shared1/total_counts/totalcounts-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã¥rets hack\n",
    "\n",
    "folders = [\"/data/shared1/bigrams/Part1\", \"/data/shared1/bigrams/Part2\",  \"/data/shared1/bigrams/Part3\", \"/data/shared1/bigrams/Part4\"] \n",
    "\n",
    "#folders = [\"/data/shared1/bigrams/part4\", \"/data/shared1/bigrams/part5\", \"/data/shared1/bigrams/part6\"]\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"delimiter\",\";\").load(folders)\n",
    "\n",
    "for i in range(0, 500):\n",
    "    df = df.withColumn('c'+str(i),split(\"_c0\",\"\\t\").getItem(i))\n",
    "    \n",
    "df = df.drop('_c0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalcounts = totalcounts.withColumnRenamed('_c0', 'year')\n",
    "totalcounts = totalcounts.withColumnRenamed('_c1', 'bigram_count')\n",
    "totalcounts = totalcounts.withColumnRenamed('_c2', 'page_count')\n",
    "totalcounts = totalcounts.withColumnRenamed('_c3', 'volume_count')\n",
    "totalcounts = totalcounts.withColumn('year', totalcounts['year'].cast('int'))\n",
    "totalcounts = totalcounts.where(totalcounts['year'] >= 1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_long(df, by):\n",
    "\n",
    "    # Filter dtypes and split into column names and type description\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
    "    \n",
    "    # Spark SQL supports only homogeneous columns\n",
    "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
    "\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = explode(array([\n",
    "      struct(lit(c).alias(\"key\"), col(c).alias(\"val\")) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "\n",
    "    return df.select(by + [kvs]).select(by + [\"kvs.key\", \"kvs.val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    split_col = pyspark.sql.functions.split(df['val'], ',')\n",
    "    df = df.withColumn('year', split_col.getItem(0))\n",
    "    df = df.withColumn('count', split_col.getItem(1))\n",
    "    df = df.withColumn('volume', split_col.getItem(2))\n",
    "\n",
    "    df = df.withColumnRenamed('c0', 'bigram')\n",
    "    \n",
    "    split_pos = pyspark.sql.functions.split(df['bigram'], ' ')\n",
    "    df = df.withColumn('bigram1', split_pos.getItem(0))\n",
    "    df = df.withColumn('bigram2', split_pos.getItem(1))\n",
    "\n",
    "    split_bigram1 = pyspark.sql.functions.split(df['bigram1'], '_')\n",
    "    df = df.withColumn('POS1', split_bigram1.getItem(1))\n",
    "    split_bigram2 = pyspark.sql.functions.split(df['bigram2'], '_')\n",
    "    df = df.withColumn('POS2', split_bigram2.getItem(1))\n",
    "\n",
    "    df = df.withColumn('year', df['year'].cast('int'))\n",
    "    df = df.withColumn('count', df['count'].cast('int'))\n",
    "    \n",
    "    df = df.drop('val')\n",
    "    df = df.drop('key')\n",
    "    df = df.drop('bigram1', 'bigram2')\n",
    "    df = df.drop('volume')\n",
    "    \n",
    "    df = df.where(df['year'] >= 1900)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "def clean(df):\n",
    "    \n",
    "    df = df.where(col('POS1').isNotNull() & col('POS2').isNotNull())\n",
    "    \n",
    "    df = df.where(((col('POS1') == 'ADJ') | (col('POS1') == 'NOUN')) &\n",
    "                  ((col('POS2') == 'ADJ') | (col('POS2') == 'NOUN')))\n",
    "    \n",
    "    df = df.where(~df.bigram.contains('_NOUN_') & ~df.bigram.contains('_ADJ_'))\n",
    "\n",
    "    df = df.withColumn('bigram_noPOS', df.bigram)\n",
    "    df = df.withColumn('bigram_noPOS', regexp_replace('bigram_noPOS', '_NOUN', ''))\n",
    "    df = df.withColumn('bigram_noPOS', regexp_replace('bigram_noPOS', '_ADJ', ''))\n",
    "    \n",
    "    # only keep bigrams consisting of two words\n",
    "    df = df.where(df.bigram_noPOS.rlike(\"\\w+ \\w+\"))\n",
    "    df = df.where(~(df.bigram_noPOS.rlike(\"[.,;:_'^?<>]+\")))\n",
    "    df = df.where(~(df.bigram_noPOS.rlike(\"[0-9]+\")))\n",
    "\n",
    "    # remove all nulls\n",
    "    df = df.withColumn('bigram', lower(col('bigram')))\n",
    "    \n",
    "    # only keep bigrams consisting of ascii chars\n",
    "    is_ascii = udf(lambda x: x.isascii(), BooleanType())\n",
    "    df = df.where(is_ascii(df.bigram_noPOS))\n",
    "    \n",
    "    df = df.drop('POS1', 'POS2', 'bigram_noPOS')\n",
    "    \n",
    "    df = df.groupBy('bigram', 'year').agg(F.sum('count').alias('count'))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = to_long(df, [\"c0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill dataset such that each bigram has an entry for each year - fill with 0 if bigram doesn't appear in given year\n",
    "\n",
    "periods = spark.range(1900, 2019+1).withColumnRenamed(\"id\", \"year\")\n",
    "unique_bigrams = df.select('bigram').distinct()\n",
    "\n",
    "periods = unique_bigrams.crossJoin(periods)\n",
    "\n",
    "full = periods.join(df, ['bigram', 'year'], how=\"full\")\n",
    "full = full.fillna({'count': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataframe containing all rows and totalcounts\n",
    "totalcounts = totalcounts.withColumnRenamed('year', 'year_totalcounts')\n",
    "df = full.join(totalcounts.select('bigram_count', 'year_totalcounts'), full.year == totalcounts.year_totalcounts, how='left')\n",
    "df = df.withColumnRenamed('bigram_count', 'total_bigrams')\n",
    "df = df.drop('year_totalcounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bigram: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- total_bigrams: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute percentages\n",
    "df = df.withColumn('bigram_percent', (F.col('count') / F.col('total_bigrams')) * 100)\n",
    "\n",
    "df = df.drop('total_bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute smooth percentages (smoothing 3)\n",
    "w = Window.partitionBy(\"bigram\").orderBy(\"year\")\n",
    "w_smooth = Window.partitionBy('bigram').orderBy('year').rowsBetween(-2, 2)\n",
    "\n",
    "df = df.withColumn('bigram_percent_smooth', avg(df.bigram_percent).over(w_smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out ngrams that never reach threshold for 3 consecutive years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5e-6\n",
    "df = df.withColumn('over_threshold', when(df.bigram_percent > threshold, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy(df.bigram).orderBy(df.year)\n",
    "w2 = Window.partitionBy(df.bigram,df.over_threshold).orderBy(df.year)\n",
    "\n",
    "df = df.withColumn('grp',F.row_number().over(w1)-F.row_number().over(w2))\n",
    "\n",
    "#Window definition for streak\n",
    "w3 = Window.partitionBy(df.bigram,df.over_threshold,df.grp).orderBy(df.year)\n",
    "streakdf = df.withColumn('streak_1',F.when(df.over_threshold == 0,0).otherwise(F.row_number().over(w3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = Window.partitionBy(df.bigram)\n",
    "cleandf = streakdf.withColumn('max_streak',F.max(col('streak_1')).over(w4)).where(col('max_streak') > 2)\n",
    "\n",
    "cleandf = cleandf.drop('over_threshold', 'grp', 'streak_1', 'max_streak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf.repartition('bigram').sortWithinPartitions('bigram', 'year').write.csv('/data/shared1/cleandata/uppercase_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
